{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f18ac7cb",
   "metadata": {},
   "source": [
    "# Project 3: Understanding User Behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c163f71",
   "metadata": {},
   "source": [
    "#### By: Shanie Hsieh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cbd085",
   "metadata": {},
   "source": [
    "As a data scientist at a game development company, we want to look at two main events of our mobile game: `buy a sword` & `join guild`. We also have other events to explore such as `buy armor` and `buy a shield`. In this notebook, we will walk through the data pipeline to catch these events, test data, and some basic analysis. This notebook will be using **Docker**, **Kafka**, **Apache Bench**, **Spark**, **Hadoop**, and **Presto**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4eb378f",
   "metadata": {},
   "source": [
    "##### **Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f838f65",
   "metadata": {},
   "source": [
    "##### Docker File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003ef0fc",
   "metadata": {},
   "source": [
    "I want to start off with initial files from Course Materials to begin my project and create the files needed in order to go through the pipeline smoothly. We copy `docker-compose.yml` from Week 13."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7052fa",
   "metadata": {},
   "source": [
    "```\n",
    "cp ~/w205/course-content/13-Understanding-Data/docker-compose.yml .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f389ad",
   "metadata": {},
   "source": [
    "This file contains the containers needed:\n",
    "- zookeeper: lets us manage kafka on the server\n",
    "- kafka: store and read past and real-time streaming data\n",
    "- cloudera: data cloud to allow us access to other tools\n",
    "- spark: helps us analyze and query data\n",
    "- presto: query engine with wider range of sql syntax than spark\n",
    "- mids: from UC Berkeley to help with command lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f131d4d0",
   "metadata": {},
   "source": [
    "##### game_api File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584c6fca",
   "metadata": {},
   "source": [
    "I also make a copy of `game_api.py` from Week 13, but also add a couple functions for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1004eab",
   "metadata": {},
   "source": [
    "```\n",
    "cp ~/w205/course-content/13-Understanding-Data/game_api.py .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaaea9ad",
   "metadata": {},
   "source": [
    "This file already contains functions for `log_to_kafka()`, `default_response()`, and `purchase_a_sword`. For analysis of `join guild`, I defined the event `join_guild()`. I also wanted to add more purchasing options so I added `purchase_armor` and `purchase_a_shield`. We want our events to be in a readable form in JSON which is then sent to Kafka to use. The `log_to_kafka()` function does this. All other functions are used for defining each event."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6789f34",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02b470f",
   "metadata": {},
   "source": [
    "##### **Spin up cluster**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976d735d",
   "metadata": {},
   "source": [
    "Now we're ready to begin going through the pipeline. Let's spin up our containers from docker-compose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edcea86",
   "metadata": {},
   "source": [
    "```\n",
    "docker-compose up -d\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e3f1dd",
   "metadata": {},
   "source": [
    "##### **Create Topic**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edabed7",
   "metadata": {},
   "source": [
    "In a new terminal, we want to create the kafka topic `events` to read the events."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2bf0dd",
   "metadata": {},
   "source": [
    "```\n",
    "# had to create topic but now broker creates the topic\n",
    "docker-compose exec kafka kafka-topics --create --topic events --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181\n",
    "# depending on version of kafka, you could use the one corresponding to your version. I used the one below.\n",
    "docker-compose exec kafka kafka-topics --describe --topic events --bootstrap-server kafka:29092\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c827c1c5",
   "metadata": {},
   "source": [
    "##### **Run Flask**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b297fe13",
   "metadata": {},
   "source": [
    "Let's start up our web app server with `game_api.py` on the Flask server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d024547b",
   "metadata": {},
   "source": [
    "```\n",
    "docker-compose exec mids env FLASK_APP=/w205/project-3-shhsieh99/game_api.py flask run --host 0.0.0.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0063579",
   "metadata": {},
   "source": [
    "We will be running this in one terminal in the background as we continue on to a new terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df06c2c",
   "metadata": {},
   "source": [
    "##### **Stream and Hive**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562777b9",
   "metadata": {},
   "source": [
    "A lot of the work from the pipeline falls here. We want to stream the data for our game into Kafka and let it move and process through Spark where it eventually ends up in Hadoop. This is all done in the file `stream_and_hive.py`. A copy of the file is reproduced below with commented cells to explain the process:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fd76d1",
   "metadata": {},
   "source": [
    "```\n",
    "# import statements\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# define the schema to streamline\n",
    "def event_schema():\n",
    "    \"\"\"\n",
    "    root\n",
    "    |-- Accept: string (nullable = true)\n",
    "    |-- Host: string (nullable = true)\n",
    "    |-- User-Agent: string (nullable = true)\n",
    "    |-- event_type: string (nullable = true)\n",
    "    |-- timestamp: string (nullable = true)\n",
    "    \"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"Accept\", StringType(), True),\n",
    "        StructField(\"Host\", StringType(), True),\n",
    "        StructField(\"User-Agent\", StringType(), True),\n",
    "        StructField(\"event_type\", StringType(), True),\n",
    "    ])\n",
    "\n",
    "# filtering events\n",
    "# This event is for all purchases a user makes, whether it me sword, armor, or shield\n",
    "@udf('boolean')\n",
    "def is_purchase(event_as_json):\n",
    "    event = json.loads(event_as_json)\n",
    "    if event['event_type'] == 'purchase_sword':\n",
    "        return True\n",
    "    elif event['event_type'] == 'purchase_armor':\n",
    "        return True\n",
    "    elif event['event_type'] == 'purchase_shield':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# This event is for a user joining a guild\n",
    "@udf('boolean')\n",
    "def is_guild_join(event_as_json):\n",
    "    event = json.loads(event_as_json)\n",
    "    if event['event_type'] == 'join_guild':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"main\n",
    "    \"\"\"\n",
    "    # starts Spark session, notice we enable Hive here\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"ExtractEventsJob\") \\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # reads stream from Kafka and loads in the data\n",
    "    raw_events = spark \\\n",
    "        .readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "        .option(\"subscribe\", \"events\") \\\n",
    "        .load()\n",
    "\n",
    "    # the next 2 chunks help filter the data nd transforms them for better processing where we create tables with all necessary information\n",
    "    all_purchases = raw_events \\\n",
    "        .filter(is_purchase(raw_events.value.cast('string'))) \\\n",
    "        .select(raw_events.value.cast('string').alias('raw_event'),\n",
    "                raw_events.timestamp.cast('string'),\n",
    "                from_json(raw_events.value.cast('string'),\n",
    "                          event_schema()).alias('json')) \\\n",
    "        .select('raw_event', 'timestamp', 'json.*')\n",
    "    guilds_join = raw_events \\\n",
    "        .filter(is_guild_join(raw_events.value.cast('string'))) \\\n",
    "        .select(raw_events.value.cast('string').alias('raw_event'),\n",
    "                raw_events.timestamp.cast('string'),\n",
    "                from_json(raw_events.value.cast('string'),\n",
    "                          event_schema()).alias('json')) \\\n",
    "        .select('raw_event', 'timestamp', 'json.*')\n",
    "    \n",
    "    # here we create the actual tables to send to HDFS\n",
    "    spark.sql(\"drop table if exists all_purchases\")\n",
    "    purchase_sql_string = \"\"\"\n",
    "        create external table if not exists all_purchases (\n",
    "            raw_event string,\n",
    "            timestamp string,\n",
    "            Accept string,\n",
    "            Host string,\n",
    "            `User-Agent` string,\n",
    "            event_type string\n",
    "            )\n",
    "            stored as parquet\n",
    "            location '/tmp/all_purchases'\n",
    "            tblproperties (\"parquet.compress\"=\"SNAPPY\")\n",
    "            \"\"\"\n",
    "    spark.sql(purchase_sql_string)   \n",
    "    spark.sql(\"drop table if exists guilds_join\")\n",
    "    guild_sql_string = \"\"\"\n",
    "        create external table if not exists guilds_join (\n",
    "            raw_event string,\n",
    "            timestamp string,\n",
    "            Accept string,\n",
    "            Host string,\n",
    "            `User-Agent` string,\n",
    "            event_type string\n",
    "            )\n",
    "            stored as parquet\n",
    "            location '/tmp/guilds_join'\n",
    "            tblproperties (\"parquet.compress\"=\"SNAPPY\")\n",
    "            \"\"\"\n",
    "    spark.sql(guild_sql_string)\n",
    "    \n",
    "    # now we send these tables to HDFS as parquets and update ever 10 seconds\n",
    "    purchase_sink = all_purchases \\\n",
    "        .writeStream \\\n",
    "        .format(\"parquet\") \\\n",
    "        .option(\"checkpointLocation\", \"/tmp/checkpoints_for_purchases\") \\\n",
    "        .option(\"path\", \"/tmp/all_purchases\") \\\n",
    "        .trigger(processingTime=\"10 seconds\") \\\n",
    "        .start()\n",
    "    guild_sink = guilds_join \\\n",
    "        .writeStream \\\n",
    "        .format(\"parquet\") \\\n",
    "        .option(\"checkpointLocation\", \"/tmp/checkpoints_for_guilds\") \\\n",
    "        .option(\"path\", \"/tmp/guilds_join\") \\\n",
    "        .trigger(processingTime=\"10 seconds\") \\\n",
    "        .start()\n",
    "     \n",
    "    # waits for queries to be termianted\n",
    "    purchase_sink.awaitTermination()\n",
    "    guild_sink.awaitTermination()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71683382",
   "metadata": {},
   "source": [
    "With this, we open a new terminal and run the code to begin writing in stream and update tables to Hive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2393ae05",
   "metadata": {},
   "source": [
    "```\n",
    "docker-compose exec spark spark-submit /w205/project-3-shhsieh99/stream_and_hive.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c512b446",
   "metadata": {},
   "source": [
    "##### **Set Up to Watch Kafka**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c74fdd0",
   "metadata": {},
   "source": [
    "Now we use kafkacat and continuously read the data and events as it's running."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abed1c33",
   "metadata": {},
   "source": [
    "```\n",
    "docker-compose exec mids kafkacat -C -b kafka:29092 -t events -o beginning\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d2754c",
   "metadata": {},
   "source": [
    "##### **Apache Bench to Generate Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3548c587",
   "metadata": {},
   "source": [
    "This is now the testing part my report. We want to generate data to use for analysis with Apache Bench. We want to generate 10 times the events for 2 users every 10 seconds. For better visualization of the code, this script is stored in `feed_data.sh`. To begin generating data, we want to run this script in a new terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4cee73",
   "metadata": {},
   "source": [
    "```\n",
    "bash feed_data.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84575e3",
   "metadata": {},
   "source": [
    "##### **See in HDFS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e878f85",
   "metadata": {},
   "source": [
    "We can make sure things are running correctly in HDFS for `all_purchases` and `join_guild` in a new terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b48ebd8",
   "metadata": {},
   "source": [
    "```\n",
    "docker-compose exec cloudera hadoop fs -ls /tmp/all_purchases\n",
    "docker-compose exec cloudera hadoop fs -ls /tmp/join_guilds\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6b1ea0",
   "metadata": {},
   "source": [
    "##### **Queries from Presto**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9452bffe",
   "metadata": {},
   "source": [
    "With the data continuously updating in HDFS, we can perform some analysis using presto. We want to first run Presto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a25856",
   "metadata": {},
   "source": [
    "```\n",
    "docker-compose exec presto presto --server presto:8080 --catalog hive --schema default\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2892a83d",
   "metadata": {},
   "source": [
    "We first check what tables currently exist in Presto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c165af63",
   "metadata": {},
   "source": [
    "```\n",
    "presto:default> show tables;\n",
    "     Table     \n",
    "---------------\n",
    " all_purchases \n",
    " guilds_join   \n",
    "(2 rows)\n",
    "\n",
    "Query 20211206_112323_00001_sja3y, FINISHED, 1 node\n",
    "Splits: 2 total, 2 done (100.00%)\n",
    "0:05 [2 rows, 66B] [0 rows/s, 12B/s]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7a55e1",
   "metadata": {},
   "source": [
    "Describe `all_purchases` table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d88f007",
   "metadata": {},
   "source": [
    "```\n",
    "presto:default> describe all_purchases;\n",
    "   Column   |  Type   | Comment \n",
    "------------+---------+---------\n",
    " raw_event  | varchar |         \n",
    " timestamp  | varchar |         \n",
    " accept     | varchar |         \n",
    " host       | varchar |         \n",
    " user-agent | varchar |         \n",
    " event_type | varchar |         \n",
    "(6 rows)\n",
    "\n",
    "Query 20211206_112350_00003_sja3y, FINISHED, 1 node\n",
    "Splits: 2 total, 1 done (50.00%)\n",
    "0:02 [6 rows, 434B] [3 rows/s, 280B/s]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15274e5",
   "metadata": {},
   "source": [
    "See total guilds joined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bf0185",
   "metadata": {},
   "source": [
    "```\n",
    "presto:default> select count(*) from guilds_join;\n",
    " _col0 \n",
    "-------\n",
    "   140 \n",
    "(1 row)\n",
    "\n",
    "Query 20211206_112432_00004_sja3y, FINISHED, 1 node\n",
    "Splits: 11 total, 8 done (72.73%)\n",
    "0:11 [110 rows, 13.8KB] [9 rows/s, 1.24KB/s]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951b4b6d",
   "metadata": {},
   "source": [
    "Grouped by Host"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7739e3",
   "metadata": {},
   "source": [
    "```\n",
    "presto:default> select Host, count(*) from guilds_join group by Host;\n",
    "       Host        | _col1 \n",
    "-------------------+-------\n",
    " user1.comcast.com |    80 \n",
    " user2.att.com     |    80 \n",
    "(2 rows)\n",
    "\n",
    "Query 20211206_112459_00005_sja3y, FINISHED, 1 node\n",
    "Splits: 14 total, 7 done (50.00%)\n",
    "0:03 [110 rows, 13.8KB] [38 rows/s, 4.89KB/s]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f18ecb",
   "metadata": {},
   "source": [
    "Lets take a look at `all_purchases` and check the count for each type of purchase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a05a414",
   "metadata": {},
   "source": [
    "```\n",
    "presto:default> select event_type, count(*) from all_purchases group by event_type;\n",
    "   event_type    | _col1 \n",
    "-----------------+-------\n",
    " purchase_sword  |   230 \n",
    " purchase_armor  |   230 \n",
    " purchase_shield |   220 \n",
    "(3 rows)\n",
    "\n",
    "Query 20211206_112545_00006_sja3y, FINISHED, 1 node\n",
    "Splits: 19 total, 13 done (68.42%)\n",
    "0:01 [480 rows, 28KB] [538 rows/s, 31.4KB/s]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5db224",
   "metadata": {},
   "source": [
    "What about the types of purchases from user 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f9fdf2",
   "metadata": {},
   "source": [
    "```\n",
    "presto:default> select event_type, count(*) from all_purchases where Host = 'user1.comcast.com' group by event_type;\n",
    "   event_type    | _col1 \n",
    "-----------------+-------\n",
    " purchase_sword  |   140 \n",
    " purchase_armor  |   140 \n",
    " purchase_shield |   140 \n",
    "(3 rows)\n",
    "\n",
    "Query 20211206_112622_00007_sja3y, FINISHED, 1 node\n",
    "Splits: 23 total, 19 done (82.61%)\n",
    "0:02 [780 rows, 45.8KB] [359 rows/s, 21.1KB/s]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4ef716",
   "metadata": {},
   "source": [
    "user 2?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e965df",
   "metadata": {},
   "source": [
    "```\n",
    "presto:default> select event_type, count(*) from all_purchases where Host = 'user2.att.com' group by event_type;\n",
    "   event_type    | _col1 \n",
    "-----------------+-------\n",
    " purchase_sword  |   150 \n",
    " purchase_armor  |   150 \n",
    " purchase_shield |   150 \n",
    "(3 rows)\n",
    "\n",
    "Query 20211206_112646_00008_sja3y, FINISHED, 1 node\n",
    "Splits: 25 total, 16 done (64.00%)\n",
    "0:01 [650 rows, 38.1KB] [607 rows/s, 35.7KB/s]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af8dd10",
   "metadata": {},
   "source": [
    "Finally, lets check that our data is constantly being generated and updated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c92d051",
   "metadata": {},
   "source": [
    "```\n",
    "presto:default> select count(*) from guilds_join;\n",
    " _col0 \n",
    "-------\n",
    "   380 \n",
    "(1 row)\n",
    "\n",
    "Query 20211206_112746_00011_sja3y, FINISHED, 1 node\n",
    "Splits: 29 total, 20 done (68.97%)\n",
    "0:01 [290 rows, 36.7KB] [303 rows/s, 38.5KB/s]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1057df97",
   "metadata": {},
   "source": [
    "We see that everything is working as it should as data is constantly being generated and updated through the pipeline for analysis. We can also see that the purchases and going through evenly and we can do analyses on purchases and guild joining. We can stop all running terminals and stop our docker-compose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa176c67",
   "metadata": {},
   "source": [
    "```\n",
    "docker-compose down\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782c22f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m78",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m78"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
